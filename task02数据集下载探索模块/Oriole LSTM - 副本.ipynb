{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n",
      "Loaded the word vectors!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "wordsList = np.load('wordsList.npy')\n",
    "print('Loaded the word list!')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load('wordVectors.npy')\n",
    "print ('Loaded the word vectors!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure everything has been loaded in correctly, we can look at the dimensions of the vocabulary list and the embedding matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "(400000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(len(wordsList))\n",
    "print(wordVectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also search our word list for a word like \"baseball\", and then access its corresponding vector through the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.075439 ,  1.2659   , -1.3179   ,  0.11341  ,  1.4513   ,\n",
       "        0.17337  , -0.56265  , -1.0706   ,  0.54898  ,  0.30163  ,\n",
       "       -0.11471  ,  0.38498  ,  0.9205   , -0.2491   ,  0.3308   ,\n",
       "        0.060113 , -0.0068846,  0.086864 , -0.20535  , -0.86098  ,\n",
       "        0.10007  , -0.75486  ,  0.48225  , -0.33253  , -0.23791  ,\n",
       "        0.17345  ,  0.49777  ,  0.88761  ,  0.089471 , -0.56217  ,\n",
       "        1.8535   , -0.0055493,  0.45845  ,  0.53943  ,  0.3247   ,\n",
       "        0.43479  , -0.027253 ,  0.44744  , -0.27514  , -0.016152 ,\n",
       "       -0.51024  , -0.10113  , -0.80985  , -0.31571  ,  1.5817   ,\n",
       "        0.2105   , -0.1844   , -1.7266   ,  0.092685 , -0.55696  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseballIndex = wordsList.index('flower')\n",
    "wordVectors[baseballIndex]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our vectors, our first step is taking an input sentence and then constructing the its vector representation. Let's say that we have the input sentence \"I thought the movie was incredible and inspiring\". In order to get the word vectors, we can use Tensorflow's embedding lookup function. This function takes in two arguments, one for the embedding matrix (the wordVectors matrix in our case), and one for the ids of each of the words. The ids vector can be thought of as the integerized representation of the training set. This is basically just the row index of each of the words. Let's look at a quick example to make this concrete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[    41    804 201534   1005     15   7446      5  13767      0      0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "maxSeqLength = 10 #Maximum length of sentence\n",
    "numDimensions = 300 #Dimensions for each word vector\n",
    "firstSentence = np.zeros((maxSeqLength), dtype='int32')\n",
    "firstSentence[0] = wordsList.index(\"i\")\n",
    "firstSentence[1] = wordsList.index(\"thought\")\n",
    "firstSentence[2] = wordsList.index(\"the\")\n",
    "firstSentence[3] = wordsList.index(\"movie\")\n",
    "firstSentence[4] = wordsList.index(\"was\")\n",
    "firstSentence[5] = wordsList.index(\"incredible\")\n",
    "firstSentence[6] = wordsList.index(\"and\")\n",
    "firstSentence[7] = wordsList.index(\"inspiring\")\n",
    "#firstSentence[8] and firstSentence[9] are going to be 0\n",
    "print(firstSentence.shape)\n",
    "print(firstSentence) #Shows the row index for each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data pipeline can be illustrated below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/SentimentAnalysis5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 10 x 50 output should contain the 50 dimensional word vectors for each of the 10 words in the sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors,firstSentence).eval().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating the ids matrix for the whole training set, let’s first take some time to visualize the type of data that we have. This will help us determine the best value for setting our maximum sequence length. In the previous example, we used a max length of 10, but this value is largely dependent on the inputs you have.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set we're going to use is the Imdb movie review dataset. This set has 25,000 movie reviews, with 12,500 positive reviews and 12,500 negative reviews. Each of the reviews is stored in a txt file that we need to parse through. The positive reviews are stored in one directory and the negative reviews are stored in another. The following piece of code will determine total and average number of words in each review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n",
      "The total number of files is 6000\n",
      "The total number of words in the files is 1391142\n",
      "The average number of words in the files is 231.857\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "positiveFiles = ['positiveReviews/' + f for f in listdir('positiveReviews/') if isfile(join('positiveReviews/', f))]\n",
    "negativeFiles = ['negativeReviews/' + f for f in listdir('negativeReviews/') if isfile(join('negativeReviews/', f))]\n",
    "numWords = []\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the Matplot library to visualize this data in a histogram format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEKCAYAAAAvlUMdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBcbMzLJwgTEzsyz+P38+hv4IMb3ZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numWords, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 1200, 0, 2000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histogram as well as the average number of words per file, we can safely say that most reviews will fall under 250 words, which is the max sequence length value we will set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxSeqLength = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can take a single file and transform it into our ids matrix. This is what one of the reviews looks like in text file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n"
     ]
    }
   ],
   "source": [
    "fname = positiveFiles[0] #Can use any valid index (not just 3)\n",
    "with open(fname) as f:\n",
    "    for lines in f:\n",
    "        print(lines)\n",
    "        exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's convert to to an ids matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([174943,    152,     14,      7,   7362,   2841,     20,   1421,\n",
       "           22, 201534,    215,     79,     19,     77,     68,   1009,\n",
       "           59,    164,    214,    125,     19,   2562,    192,   1678,\n",
       "           82,      6, 201534,   3174,   8104,    410,    285,      4,\n",
       "          733,     12, 174943,   7984,  15303,     14,    181,   2386,\n",
       "            4,   2532,     73,     14,   2562, 201534,  14170,      4,\n",
       "         3981,   7980, 201534,  34401,    543,     38,     86,    253,\n",
       "          248,    131,     44,  22495,   2562,  31166, 201534,  91887,\n",
       "            3, 201534,   1115,    794,     64,   9794,    285,      3,\n",
       "       201534,    888,     41,   1522,      5,     44,    543,     61,\n",
       "           41,    822, 201534,   1942,      6,     42,      7,   1283,\n",
       "         2648,    977,      4,   6292,    135, 201534,    164,     41,\n",
       "         1040,   3151,     22,    152,      7,   2392,    331,   5537,\n",
       "        14663,    187,      4,  11739,     48,      3,    392,   2562,\n",
       "         1283,   3143,      4, 174943,    152,     41,   1543,     12,\n",
       "          109,   3574,      3,    192,    464,    269,     12, 174943,\n",
       "          152,     14,    372,  19386,    102,      7,  16214,     12,\n",
       "           20, 228955,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstFile = np.zeros((maxSeqLength), dtype='int32')\n",
    "with open(fname) as f:\n",
    "    indexCounter = 0\n",
    "    line=f.readline()\n",
    "    cleanedLine = cleanSentences(line)\n",
    "    split = cleanedLine.split()\n",
    "    for word in split:\n",
    "        if indexCounter < maxSeqLength:\n",
    "            try:\n",
    "                firstFile[indexCounter] = wordsList.index(word)\n",
    "            except ValueError:\n",
    "                firstFile[indexCounter] = 399999 #Vector for unknown words\n",
    "        indexCounter = indexCounter + 1\n",
    "firstFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the same for each of our 25,000 reviews. We'll load in the movie training set and integerize it to get a 25000 x 250 matrix. This was a computationally expensive process, so instead of having you run the whole piece, we’re going to load in a pre-computed IDs matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "# fileCounter = 0\n",
    "# for pf in positiveFiles:\n",
    "#    with open(pf, \"r\") as f:\n",
    "#        indexCounter = 0\n",
    "#        line=f.readline()\n",
    "#        cleanedLine = cleanSentences(line)\n",
    "#        split = cleanedLine.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#            except ValueError:\n",
    "#                ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#            indexCounter = indexCounter + 1\n",
    "#            if indexCounter >= maxSeqLength:\n",
    "#                break\n",
    "#        fileCounter = fileCounter + 1 \n",
    "\n",
    "# for nf in negativeFiles:\n",
    "#    with open(nf, \"r\") as f:\n",
    "#        indexCounter = 0\n",
    "#        line=f.readline()\n",
    "#        cleanedLine = cleanSentences(line)\n",
    "#        split = cleanedLine.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#            except ValueError:\n",
    "#                ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#            indexCounter = indexCounter + 1\n",
    "#            if indexCounter >= maxSeqLength:\n",
    "#                break\n",
    "#        fileCounter = fileCounter + 1 \n",
    "# #Pass into embedding function and see if it evaluates. \n",
    "\n",
    "# np.save('idsMatrix', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids = np.load('idsMatrix.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can find a couple of helper functions that will be useful when training the network in a later step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0): \n",
    "            num = randint(0,2700)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            num = randint(3300,5999)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(2700,3300)\n",
    "        if (num <= 2999):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we’re ready to start creating our Tensorflow graph. We’ll first need to define some hyperparameters, such as batch size, number of LSTM units, number of output classes, and number of training iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with most Tensorflow graphs, we’ll now need to specify two placeholders, one for the inputs into the network, and one for the labels. The most important part about defining these placeholders is understanding each of their dimensionalities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels placeholder represents a set of values, each either [1, 0] or [0, 1], depending on whether each training example is positive or negative. Each row in the integerized input placeholder represents the integerized representation of each training example that we include in our batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/SentimentAnalysis12.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our input data placeholder, we’re going to call the tf.nn.lookup() function in order to get our word vectors. The call to that function will return a 3-D Tensor of dimensionality batch size by max sequence length by word vector dimensions. In order to visualize this 3-D tensor, you can simply think of each data point in the integerized input tensor as the corresponding D dimensional vector that it refers to. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/SentimentAnalysis13.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data in the format that we want, let’s look at how we can feed this input into an LSTM network. We’re going to call the tf.nn.rnn_cell.BasicLSTMCell function. This function takes in an integer for the number of LSTM units that we want. This is one of the hyperparameters that will take some tuning to figure out the optimal value. We’ll then wrap that LSTM cell in a dropout layer to help prevent the network from overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we’ll feed both the LSTM cell and the 3-D tensor full of input data into a function called tf.nn.dynamic_rnn. This function is in charge of unrolling the whole network and creating a pathway for the data to flow through the RNN graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a side note, another more advanced network architecture choice is to stack multiple LSTM cells on top of each other. This is where the final hidden state vector of the first LSTM feeds into the second. Stacking these cells is a great way to help the model retain more long term dependence information, but also introduces more parameters into the model, thus possibly increasing the training time, the need for additional training examples, and the chance of overfitting. For more information on how you can add stacked LSTMs to your model, check out Tensorflow's excellent [documentation](https://www.tensorflow.org/tutorials/recurrent#stacking_multiple_lstms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first output of the dynamic RNN function can be thought of as the last hidden state vector. This vector will be reshaped and then multiplied by a final weight matrix and a bias term to obtain the final output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "#取最终的结果值\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll define correct prediction and accuracy metrics to track how the network is doing. The correct prediction formulation works by looking at the index of the maximum value of the 2 output values, and then seeing whether it matches with the training labels. \n",
    "\n",
    "接下来，我们需要定义正确的预测函数和正确率评估参数。正确的预测形式是查看最后输出的0-1向量是否和标记的0-1向量相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll define a standard cross entropy loss with a softmax layer put on top of the final prediction values. For the optimizer, we’ll use Adam and the default learning rate of .001. \n",
    "\n",
    "之后，我们使用一个标准的交叉熵损失函数来作为损失值。对于优化器，我们选择 Adam，并且采用默认的学习率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’d like to use Tensorboard to visualize the loss and accuracy values, you can also run and the modify the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard4-7-1/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the right values for your hyperparameters is a crucial part of training deep neural networks effectively. You'll find that your training loss curves can vary with your choice of optimizer (Adam, Adadelta, SGD, etc), learning rate, and network architecture. With RNNs and LSTMs in particular, some other important factors include the number of LSTM units and the size of the word vectors.\n",
    "\n",
    "* Learning Rate: RNNs are infamous for being diffult to train because of the large number of time steps they have. Learning rate becomes extremely important since we don't want our weight values to fluctuate wildly as a result of a large learning rate, nor do we want a slow training process due to a low learning rate. The default value of 0.001 is a good place to start. You should increase this value if the training loss is changing very slowly, and decrease if the loss is unstable.  \n",
    "* Optimizer: There isn't a consensus choice among researchers, but Adam has been widely popular due to having the adaptive learning rate property (Keep in mind that optimal learning rates can differ with the choice of optimizer).\n",
    "* Number of LSTM units: This value is largely dependent on the average length of your input texts. While a greater number of units provides more expressibility for the model and allows the model to store more information for longer texts, the network will take longer to train and will be computationally expensive. \n",
    "* Word Vector Size: Dimensions for word vectors generally range from 50 to 300. A larger size means that the vector is able to encapsulate more information about the word, but you should also expect a more computationally expensive model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of the training loop is that we first define a Tensorflow session. Then, we load in a batch of reviews and their associated labels. Next, we call the session’s `run` function. This function has two arguments. The first is called the \"fetches\" argument. It defines the value we’re interested in computing. We want our optimizer to be computed since that is the component that minimizes our loss function. The second argument is where we input our `feed_dict`. This data structure is where we provide inputs to all of our placeholders. We need to feed our batch of reviews and our batch of labels. This loop is then repeated for a set number of training iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training the network in this notebook (which will take at least a couple of hours), we’ll load in a pretrained model.\n",
    "\n",
    "If you decide to train this notebook on your own machine, note that you can track its progress using [TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard). While the following cell is running, use your terminal to enter the directory that contains this notebook, enter `tensorboard --logdir=tensorboard`, and visit http://localhost:6006/ with a browser to keep an eye on your training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:1/16000 \n",
      "loss:0.7575618624687195 \n",
      "accuracy:50.0\n",
      "..........\n",
      "iteration:501/16000 \n",
      "loss:0.7101984024047852 \n",
      "accuracy:58.33333134651184\n",
      "..........\n",
      "iteration:1001/16000 \n",
      "loss:0.6529808044433594 \n",
      "accuracy:50.0\n",
      "..........\n",
      "saved to models4-7-1/pretrained_lstm.ckpt-1000\n",
      "iteration:1501/16000 \n",
      "loss:0.7020056247711182 \n",
      "accuracy:62.5\n",
      "..........\n",
      "iteration:2001/16000 \n",
      "loss:0.7107117176055908 \n",
      "accuracy:50.0\n",
      "..........\n",
      "saved to models4-7-1/pretrained_lstm.ckpt-2000\n",
      "iteration:2501/16000 \n",
      "loss:0.5790966749191284 \n",
      "accuracy:58.33333134651184\n",
      "..........\n",
      "iteration:3001/16000 \n",
      "loss:0.7200605869293213 \n",
      "accuracy:45.83333432674408\n",
      "..........\n",
      "saved to models4-7-1/pretrained_lstm.ckpt-3000\n",
      "iteration:3501/16000 \n",
      "loss:0.589256227016449 \n",
      "accuracy:62.5\n",
      "..........\n",
      "iteration:4001/16000 \n",
      "loss:0.5020411610603333 \n",
      "accuracy:62.5\n",
      "..........\n",
      "saved to models4-7-1/pretrained_lstm.ckpt-4000\n",
      "iteration:4501/16000 \n",
      "loss:0.373099684715271 \n",
      "accuracy:70.83333134651184\n",
      "..........\n",
      "iteration:5001/16000 \n",
      "loss:0.38242673873901367 \n",
      "accuracy:75.0\n",
      "..........\n",
      "saved to models4-7-1/pretrained_lstm.ckpt-5000\n",
      "iteration:5501/16000 \n",
      "loss:0.5627119541168213 \n",
      "accuracy:62.5\n",
      "..........\n",
      "iteration:6001/16000 \n",
      "loss:0.38066521286964417 \n",
      "accuracy:75.0\n",
      "..........\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "saved to models4-7-1/pretrained_lstm.ckpt-6000\n",
      "iteration:6501/16000 \n",
      "loss:0.5041630268096924 \n",
      "accuracy:70.83333134651184\n",
      "..........\n",
      "iteration:7001/16000 \n",
      "loss:0.29659414291381836 \n",
      "accuracy:79.16666865348816\n",
      "..........\n",
      "saved to models4-7-1/pretrained_lstm.ckpt-7000\n",
      "iteration:7501/16000 \n",
      "loss:0.35347509384155273 \n",
      "accuracy:87.5\n",
      "..........\n",
      "iteration:8001/16000 \n",
      "loss:0.47452783584594727 \n",
      "accuracy:70.83333134651184\n",
      "..........\n",
      "saved to models4-7-1/pretrained_lstm.ckpt-8000\n",
      "iteration:8501/16000 \n",
      "loss:0.4040625989437103 \n",
      "accuracy:75.0\n",
      "..........\n",
      "iteration:9001/16000 \n",
      "loss:0.38115230202674866 \n",
      "accuracy:79.16666865348816\n",
      "..........\n",
      "saved to models4-7-1/pretrained_lstm.ckpt-9000\n",
      "iteration:9501/16000 \n",
      "loss:0.4734853208065033 \n",
      "accuracy:54.16666865348816\n",
      "..........\n",
      "iteration:10001/16000 \n",
      "loss:0.3989998400211334 \n",
      "accuracy:70.83333134651184\n",
      "..........\n",
      "saved to models4-7-1/pretrained_lstm.ckpt-10000\n",
      "iteration:10501/16000 \n",
      "loss:0.40243029594421387 \n",
      "accuracy:75.0\n",
      "..........\n",
      "iteration:11001/16000 \n",
      "loss:0.23304937779903412 \n",
      "accuracy:87.5\n",
      "..........\n",
      "saved to models4-7-1/pretrained_lstm.ckpt-11000\n",
      "iteration:11501/16000 \n",
      "loss:0.32790908217430115 \n",
      "accuracy:87.5\n",
      "..........\n",
      "iteration:12001/16000 \n",
      "loss:0.5245754718780518 \n",
      "accuracy:62.5\n",
      "..........\n",
      "saved to models4-7-1/pretrained_lstm.ckpt-12000\n",
      "iteration:12501/16000 \n",
      "loss:0.21716642379760742 \n",
      "accuracy:91.66666865348816\n",
      "..........\n",
      "iteration:13001/16000 \n",
      "loss:0.2293960303068161 \n",
      "accuracy:87.5\n",
      "..........\n",
      "saved to models4-7-1/pretrained_lstm.ckpt-13000\n",
      "iteration:13501/16000 \n",
      "loss:0.3113730549812317 \n",
      "accuracy:83.33333134651184\n",
      "..........\n",
      "iteration:14001/16000 \n",
      "loss:0.16733868420124054 \n",
      "accuracy:95.83333134651184\n",
      "..........\n",
      "saved to models4-7-1/pretrained_lstm.ckpt-14000\n",
      "iteration:14501/16000 \n",
      "loss:0.298645943403244 \n",
      "accuracy:79.16666865348816\n",
      "..........\n",
      "iteration:15001/16000 \n",
      "loss:0.12315681576728821 \n",
      "accuracy:91.66666865348816\n",
      "..........\n",
      "saved to models4-7-1/pretrained_lstm.ckpt-15000\n",
      "iteration:15501/16000 \n",
      "loss:0.19998657703399658 \n",
      "accuracy:91.66666865348816\n",
      "..........\n"
     ]
    }
   ],
   "source": [
    "# sess = tf.InteractiveSession()\n",
    "# saver = tf.train.Saver()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(iterations):\n",
    "   #Next Batch of reviews\n",
    "    nextBatch, nextBatchLabels = getTrainBatch();\n",
    "\n",
    "    sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "   \n",
    "   #Write summary to Tensorboard\n",
    "    if (i % 50 == 0):\n",
    "        summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        writer.add_summary(summary, i)\n",
    "        \n",
    "    if (i%500==0):\n",
    "        loss_ = sess.run(loss, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        accuracy_=(sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100\n",
    "        print(\"iteration:{}/{}\".format(i+1, iterations),\n",
    "                  \"\\nloss:{}\".format(loss_),\n",
    "                  \"\\naccuracy:{}\".format(accuracy_))    \n",
    "        print('..........')  \n",
    "\n",
    "   #Save the network every 10,000 training iterations\n",
    "    if (i % 1000 == 0 and i != 0):\n",
    "        save_path = saver.save(sess, \"models4-7-1/pretrained_lstm.ckpt\", global_step=i)\n",
    "        print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading a Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pretrained model’s accuracy and loss curves during training can be found below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/SentimentAnalysis6.png)\n",
    "![caption](Images/SentimentAnalysis7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the training curves above, it seems that the model's training is going well. The loss is decreasing steadily, and the accuracy is approaching 100 percent. However, when analyzing training curves, we should also pay special attention to the possibility of our model overfitting the training dataset. Overfitting is a common phenomenon in machine learning where a model becomes so fit to the training data that it loses the ability to generalize to the test set. This means that training a network until you achieve 0 training loss might not be the best way to get an accurate model that performs well on data it has never seen before. Early stopping is an intuitive technique commonly used with LSTM networks to combat this issue. The basic idea is that we train the model on our training set, while also measuring its performance on the test set every now and again. Once the test error stops its steady decrease and begins to increase instead, you'll know to stop training, since this is a sign that the network has begun to overfit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a pretrained model involves defining another Tensorflow session, creating a Saver object, and then using that object to call the restore function. This function takes into 2 arguments, one for the current session, and one for the name of the saved model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models4-7-1/pretrained_lstm.ckpt-15000\n"
     ]
    }
   ],
   "source": [
    "# sess = tf.InteractiveSession()\n",
    "# saver = tf.train.Saver()\n",
    "model_file=tf.train.latest_checkpoint('models4-7-1/')\n",
    "\n",
    "saver.restore(sess, model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models4-7-1/pretrained_lstm.ckpt-12000\n"
     ]
    }
   ],
   "source": [
    "# sess = tf.InteractiveSession()\n",
    "# saver = tf.train.Saver()\n",
    "model_file=\"models4-7-1/pretrained_lstm.ckpt-12000\"\n",
    "#model_file=tf.train.latest_checkpoint('models4-7-1/')\n",
    "\n",
    "saver.restore(sess, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we’ll load some movie reviews from our test set. Remember, these are reviews that the model has not been trained on and has never seen before. The accuracy for each test batch can be seen when you run the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this batch: 45.83333432674408\n",
      "Accuracy for this batch: 70.83333134651184\n",
      "Accuracy for this batch: 41.66666567325592\n",
      "Accuracy for this batch: 45.83333432674408\n",
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 45.83333432674408\n",
      "Accuracy for this batch: 33.33333432674408\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 79.16666865348816\n",
      "Accuracy for this batch: 50.0\n"
     ]
    }
   ],
   "source": [
    "#14000\n",
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 70.83333134651184\n",
      "Accuracy for this batch: 62.5\n",
      "Accuracy for this batch: 45.83333432674408\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 45.83333432674408\n",
      "Accuracy for this batch: 41.66666567325592\n",
      "Accuracy for this batch: 58.33333134651184\n",
      "Accuracy for this batch: 58.33333134651184\n",
      "Accuracy for this batch: 62.5\n"
     ]
    }
   ],
   "source": [
    "#13000\n",
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 41.66666567325592\n",
      "Accuracy for this batch: 41.66666567325592\n",
      "Accuracy for this batch: 45.83333432674408\n",
      "Accuracy for this batch: 45.83333432674408\n",
      "Accuracy for this batch: 54.16666865348816\n",
      "Accuracy for this batch: 54.16666865348816\n",
      "Accuracy for this batch: 70.83333134651184\n",
      "Accuracy for this batch: 41.66666567325592\n",
      "Accuracy for this batch: 50.0\n"
     ]
    }
   ],
   "source": [
    "#15000\n",
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we went over a deep learning approach to sentiment analysis. We looked at the different components involved in the whole pipeline and then looked at the process of writing Tensorflow code to implement the model in practice. Finally, we trained and tested the model so that it is able to classify movie reviews.\n",
    "\n",
    "With the help of Tensorflow, you can create your own sentiment classifiers to understand the large amounts of natural language in the world, and use the results to form actionable insights. Thanks for reading and following along!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
