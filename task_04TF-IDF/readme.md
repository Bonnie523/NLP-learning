##  1. TF-IDF原理。   
让我们从一个实例开始讲起。假定现在有一篇长文《中国的蜜蜂养殖》，我们准备用计算机提取它的关键词。  
如果某个词很重要，它应该在这篇文章中多次出现。于是，我们进行"词频"（Term Frequency，缩写为TF）统计。   
"停用词"（stop words），表示对找到结果毫无帮助、必须过滤掉的词。   
假设我们把它们都过滤掉了，只考虑剩下的有实际意义的词。这样又会遇到了另一个问题，我们可能发现"中国"、"蜜蜂"、"养殖"这三个词的出现次数一样多。这是不是意味着，作为关键词，它们的重要性是一样的？   
显然不是这样。因为"中国"是很常见的词，相对而言，"蜜蜂"和"养殖"不那么常见。如果这三个词在一篇文章的出现次数一样多，有理由认为，"蜜蜂"和"养殖"的重要程度要大于"中国"，也就是说，在关键词排序上面，"蜜蜂"和"养殖"应该排在"中国"的前面。   
**如果某个词比较少见，但是它在这篇文章中多次出现，那么它很可能就反映了这篇文章的特性，正是我们所需要的关键词。**  
用统计学语言表达，就是在词频的基础上，要对每个词分配一个"重要性"权重。最常见的词（"的"、"是"、"在"）给予最小的权重，较常见的词（"中国"）给予较小的权重，较少见的词（"蜜蜂"、"养殖"）给予较大的权重。这个权重叫做"逆文档频率"（Inverse Document Frequency，缩写为IDF），它的大小与一个词的常见程度成反比。   
>知道了"词频"（TF）和"逆文档频率"（IDF）以后，将这两个值相乘，就得到了一个词的TF-IDF值。某个词对文章的重要性越高，它的TF-IDF值就越大。所以，排在最前面的几个词，就是这篇文章的关键词。   
## 2. 文本矩阵化，使用词袋模型，以TF-IDF特征值为权重。（可以使用Python中TfidfTransformer库）  
```python
from sklearn.feature_extraction.text import TfidfTransformer  
from sklearn.feature_extraction.text import CountVectorizer  
corpus=["I come to China to travel", 
    "This is a car polupar in China",          
    "I love tea and Apple ",   
    "The work is to write some papers in science"] 
#将文本中的词语转换为词频矩阵
vectorizer=CountVectorizer()
#计算个词语出现的次数 
print (vectorizer.fit_transform(corpus))
print (vectorizer.fit_transform(corpus).toarray())
#获取词袋中所有文本关键词  
print (vectorizer.get_feature_names())
#进行向量化，TF-IDF和标准化三步处理
transformer = TfidfTransformer()

tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))  
print (tfidf)
#文本矩阵化，以TF-IDF特征值为权重，只需将上边得到的tfidf结果转换为矩阵即可；
array = tfidf.toarray()
print (array)
#===================================================
(0, 16)       1
  (0, 3)        1
  (0, 15)       2
  (0, 4)        1
  (1, 5)        1
  (1, 9)        1
  (1, 2)        1
  (1, 6)        1
  (1, 14)       1
  (1, 3)        1
  (2, 1)        1
  (2, 0)        1
  (2, 12)       1
  (2, 7)        1
  (3, 10)       1
  (3, 8)        1
  (3, 11)       1
  (3, 18)       1
  (3, 17)       1
  (3, 13)       1
  (3, 5)        1
  (3, 6)        1
  (3, 15)       1
[[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0]
 [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0]
 [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]
 [0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1]]
['and', 'apple', 'car', 'china', 'come', 'in', 'is', 'love', 'papers', 'polupar', 'science', 'some', 'tea', 'the', 'this', 'to', 'travel', 'work', 'write']
  (0, 16)       0.442462137895
  (0, 15)       0.697684463384
  (0, 4)        0.442462137895
  (0, 3)        0.348842231692
  (1, 14)       0.453386397373
  (1, 9)        0.453386397373
  (1, 6)        0.357455043342
  (1, 5)        0.357455043342
  (1, 3)        0.357455043342
  (1, 2)        0.453386397373
  (2, 12)       0.5
  (2, 7)        0.5
  (2, 1)        0.5
  (2, 0)        0.5
  (3, 18)       0.356579823338
  (3, 17)       0.356579823338
  (3, 15)       0.281131628441
  (3, 13)       0.356579823338
  (3, 11)       0.356579823338
  (3, 10)       0.356579823338
  (3, 8)        0.356579823338
  (3, 6)        0.281131628441
  (3, 5)        0.281131628441
[[ 0.          0.          0.          0.34884223  0.44246214  0.          0.
   0.          0.          0.          0.          0.          0.          0.
   0.          0.69768446  0.44246214  0.          0.        ]
 [ 0.          0.          0.4533864   0.35745504  0.          0.35745504
   0.35745504  0.          0.          0.4533864   0.          0.          0.
   0.          0.4533864   0.          0.          0.          0.        ]
 [ 0.5         0.5         0.          0.          0.          0.          0.
   0.5         0.          0.          0.          0.          0.5         0.
   0.          0.          0.          0.          0.        ]
 [ 0.          0.          0.          0.          0.          0.28113163
   0.28113163  0.          0.35657982  0.          0.35657982  0.35657982
   0.          0.35657982  0.          0.28113163  0.          0.35657982
   0.35657982]]

```
[刘建平文本挖掘之TF-IDF](https://www.cnblogs.com/pinard/p/6693230.html)   
## 3. 互信息的原理。  
![images/task_04TF-IDF](互信息图.jpg)   
其衡量的是两个随机变量之间的相关性，即一个随机变量中包含的关于另一个随机变量的信息量。所谓的随机变量，即随机试验结
果的量的表示，可以简单理解为按照一个概率分布进行取值的变量，比如随机抽查的一个人的身高就是一个随机变量。
可以看出，互信息其实就是对X和Y的所有可能的取值情况的点互信息PMI的加权和。因此，点互信息这个名字还是很形象的。   
 
## 4. 使用第二步生成的特征矩阵，利用互信息进行特征筛选。  
```
from sklearn import metrics as mr
mr.mutual_info_score(label,x)
```
label、x为list或array。
计算x和label的互信息。  
***  
参考资料 
文本挖掘预处理之TF-IDF：文本挖掘预处理之TF-IDF - 刘建平Pinard - 博客园 (https://www.cnblogs.com/pinard/p/6693230.html)   
使用不同的方法计算TF-IDF值：使用不同的方法计算TF-IDF值 - 简书(https://www.jianshu.com/p/f3b92124cd2b)   
sklearn-点互信息和互信息：sklearn：点互信息和互信息 - 专注计算机体系结构 - CSDN博客 (https://blog.csdn.net/u013710265/article/details/72848755)   
如何进行特征选择（理论篇）机器学习你会遇到的“坑”：如何进行特征选择（理论篇）机器学习你会遇到的“坑” (https://baijiahao.baidu.com/s?id=1604074325918456186&wfr=spider&for=pc)
