## 【任务3 - 特征提取】时长：2天  
***  
### 1.基本文本处理技能        
#### 1.1 分词的概念   
中文分词(Chinese Word Segmentation) 指的是将一个汉字序列切分成一个一个单独的词。分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。    
由于计算机只认识数字符号，对于英文、汉字这类“高级语言”计算机还没考过1级证书。而对于一句话（sentence）组成的基本单位就是字和词。想要对一句话进行编码成计算机能认识的话，则第一步就需要讲一句话按照语义切分成字和词，
然后单独处理这些字和词就方便得多了。这个便是分词。

分词算法设计中的几个基本原则：  
1、颗粒度越大越好：用于进行语义分析的文本分词，要求分词结果的颗粒度越大，即单词的字数越多，所能表示的含义越确切。    
2、切分结果中非词典词越少越好。“技术 和 服务”有0个非词典词，因此选用后者。   
3、总体词数越少越好，在相同字数的情况下，总词数越少，说明语义单元越少，那么相对的单个语义单元的权重会越大，因此准确性会越高。    
***
下面详细说说正向最大匹配法、逆向最大匹配法和双向最大匹配法具体是如何进行的：    
先说说什么是最大匹配法：最大匹配是指以词典为依据，取词典中最长单词为第一次 取字数量的扫描串，在词典中进行扫描。    
例如：词典中最长词为“中华人民共和国”共7个汉字，则最大匹配起始字数为7个汉字。然后逐字递减，在对应的词典中进行查找。    

a、正向最大匹配法，是一种分治+贪婪的思想，并不一次处理全部串，而是分别处理预先设立长度的每一段，在每一段中求取最长的并且出现在字典里面的词。  
* 例如：abcdef，预先设立的最大长度为3。所以，先从串的开始截取长度为三的子串，即abc，如果abc出现在字典中，那么abc将作为分词结果，
接着以相同方式处理def；   
* 如果abc没出现在字典里面，则从右边减少一个字符，再次匹配字典，即ab匹配，减少的字符将加入之前未选择的字符集里面作下一次匹配，
这里是cdef，如果一个串没匹配到长度大于1的字典词，则返回最左边的字符作为该串的分词结果，也就是ab如果没有匹配到，无论a是否在字典中，都将作为分词结果。

b、有了正向最大分词，逆向就很好理解了，正向是从前向后选取最大长度的串，然后从选取串的尾部向前匹配字典词，删除右边的字符。
逆向最大便是从后向前选取最大长度的串，从选取串开始向后匹配字典词，而删减的也便是左边的字符。    

c、双向最大匹配法：   
正向最大匹配法和逆向最大匹配法，都有其局限性因此有人又提出了双向最大匹配法，双向最大匹配法。
即，两种算法都切一遍，然后根据大颗粒度词越多越好，非词典词和单字词越少越好的原则，选取其中一种分词结果输出。  
如：“我们在野生动物园玩”  
正向最大匹配法，最终切分结果为：“我们/在野/生动/物/园/玩”，其中，两-字词有3个，单字字典词有2个，非词典词为1。   
逆向最大匹配法，最终切分结果为：“我们/在/野生动物园/玩”，其中，五字词有1个，两字词有1个，单字字典词为2，非词典词为0。   
非字典词：正向(1)>逆向(0)（越少越好）    
单字字典词：正向(2)=逆向(2)（越少越好）   
总词数：正向(6)>逆向(4)（越少越好）   
因此最终输出为逆向结果。  

#### 1.2 词、字符频率统计；   
（可以使用Python中的collections.Counter模块，也可以自己寻找其他好用的库）
```
from collections import Counter
s = "我/是/一个/测试/句子/，/大家/赶快/来/统计/我/吧/，/大家/赶快/来/统计/我/吧/，/大家/赶快/来/统计/我/吧/，/重要/事情/说/三遍/！/"
s_list = s.split('/') 
[s_list.remove(item) for item in s_list if item in '，。！”“']
list1 = Counter(s_list)
print(list1)
```   
>Counter({'我': 4, '大家': 3, '赶快': 3, '来': 3, '统计': 3, '吧': 3, '是': 1, '一个': 1, '测试': 1, '句子': 1, '重要': 1, '事情': 1, '说': 1, '三遍': 1, '': 1})    
[参考代码](https://www.cnblogs.com/hycstar/p/9345751.html)  

### 2. 语言分析   
#### 2.1语言模型中的的unigram、bigram、trigram   
语言模型就是用来计算一个句子的概率的模型，即P(W1,W2,W3....Wk)   
* 利用语言模型，可以确定哪个词序列的可能性更大，或者给定若干个词，可以预测下一个最可能出现的词语。举个音字转换的例子来说，输入拼音串为nixianzaiganshenme，对应的输出可以有多种形式，如你现在干什么、你西安再赶什么、等等，那么到底哪个才是正确的转换结果呢，利用语言模型，我们知道前者的概率大于后者，因此转换成前者在多数情况下比较合理。再举一个机器翻译的例子，给定一个汉语句子为李明正在家里看电视，可以翻译为Li Ming is watching TV at home、Li Ming at home is watching TV、等等，同样根据语言模型，我们知道前者的概率大于后者，所以翻译成前者比较合理。   

那么如何计算一个句子的概率呢？给定句子（词语序列）  
S(W1,W2,W3....Wk)   
它的概率可以表示为：    
P(S) = P(W1,W2,W3....Wk) =  P(W1)P(W2|W1)P(W3|W1,W2)....P(Wk|W1,W2,..Wk-1)  （1）
由于上式中的参数过多，因此需要近似的计算方法。常见的方法有n-gram模型方法、决策树方法、最大熵模型方法、最大熵马尔科夫模型方法、条件随机域方法、神经网络方法，等等。   
##### N-gram模型    
n-gram模型也称为n-1阶马尔科夫模型，它有一个有限历史假设：当前词的出现概率仅仅与前面n-1个词相关。   
P(S) = P(W1,W2,W3....Wk) =  连乘  P(Wi|Wi-n+1,....Wi-1)   
当n取1、2、3时，n-gram模型分别称为unigram、bigram和trigram语言模型。   

#### 2.2 unigram、bigram频率统计  
unigram 一元分词，把句子分成一个一个的汉字   
bigram 二元分词，把句子从头到尾每两个字组成一个词语   
trigram 三元分词，把句子从头到尾每三个字组成一个词语   

### 3. 文本矩阵化  
（要求采用词袋模型且是词级别的矩阵化）
#### 3.1分词（jieba）  
```
# encoding=utf-8
import jieba
seg_list = jieba.cut("我来到北京清华大学", cut_all=True)
print("全模式: " + "/ ".join(seg_list))  # 全模式

seg_list = jieba.cut("我来到北京清华大学", cut_all=False)
print("精确模式（默认）: " + "/ ".join(seg_list))  # 精确模式

#全模式: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学
#精确模式（默认）: 我/ 来到/ 北京/ 清华大学
```   
[jieba - github](https://github.com/fxsjy/jieba)
#### 3.2 去停用词；构造词表   


