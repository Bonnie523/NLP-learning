## 1. word2vec
&emsp;word2vec作为神经概率语言模型的输入，其本身其实是神经概率模型的副产品，是为了通过神经网络学习某个语言模型而产生的中间结果。
具体来说，“某个语言模型”指的是“CBOW”和“Skip-gram”。具体学习过程会用到两个降低复杂度的近似方法——Hierarchical Softmax或Negative Sampling。
两个模型乘以两种方法，一共有四种实现。这些内容就是本文理论部分要详细阐明的全部了。   
![CBOW](./images/CBOW.png)   
[word2vec原理推导与代码分析](http://www.hankcs.com/nlp/word2vec.html#h2-0)    
******    
来自刘建平博客评论---   
&emsp;2\\\您好！skip-gram是输入target，预测context，那请问它的输入是什么形式？您说输入层对应的是一个神经元？那是target作为1X1矩阵输入？毕竟在我的理解中输入层神经元节点数对应着输入向量的维度。  
&emsp;#8楼[楼主] 2018-07-01 22:27 刘建平Pinard    
你好，输入就是一个词向量，输出是若干词向量。如果词向量维度为5， 上下文一共8个词，那么输入就是一个词，向量维度为5， 输出就是8个词，每个词的向量维度为5.   
&emsp;word2vec里的节点定义和DNN的还是稍有不同，它没有完全按照DNN的模型来建立神经元，即你认为的可能输入是5个神经元。具体你可以看word2vec的代码就知道了，代码我在后面几篇有链接。   
******   
&emsp;1  你好，cbow和skip-gram 输入的词向量是One hot representation表示的维度很大的词向量，然后输出是像Dristributed representation表示的维度较短的词向量？是这样理解吗？     
&emsp;#13楼[楼主] 2018-08-03 13:18 刘建平Pinard    
你好，不是的。  
cbow和skip-gram 输 入的词向量是随机初始化的维度较短的词向量，优化后，会成为最终的维度较短的词向量。也就是初始化和最后的结果词向量的维度一样。    
*******     
&emsp;3  问题1：如果输入的词向量是随机初始化的话，那么是对整个词典的词向量进行随机初始化，然后进行训练迭代，使得任一词作为输入和输出时词向量一致时训练结束，我这样理解对吗？   
&emsp;问题3：有些课程和博客说输入词向量是one-hot向量，实际word2vec是用那种向量作输入呢？    
&emsp;#29楼[楼主] 2018-09-17 11:00 刘建平Pinard     
1. 是的，整个语料库的词向量都做随机初始化。不过迭代结束不需要严格达到任一词作为输入和输出时词向量一致，而是满足改变的程度小于某一个极小的范围即可认为收敛。或者说新的词向量和老的词向量的欧式距离小于一定程度即可认为收敛。   
3. word2vec不使用onehot向量，它的向量维度需要自己指定，比onehot的维度会小得多。   
-----
&emsp;4- @ 刘建平Pinard
您好，博主。麻烦问下“最早的词向量神经网络是onehot的，后面慢慢也开始有低于onehot维度的词向量训练出现。不过这个小一些的维度也是自己定义的，由于自己定义，所以词向量就要随机初始化。”也就说现在的词向量神经网络多数不使用onehot的词向量作为网络的输入，更多的是采用随机化一个随机初始化的维度较短的词向量？这样做的原因是onehot的词向量维度太大，作为网络输入太复杂？但是随机初始化的词向量相比onehot词向量输入，是不是收敛性会差一些？        
&emsp;#40楼[楼主] 2019-01-15 10:58 刘建平Pinard   
你好！   
”也就说现在的词向量神经网络多数不使用onehot的词向量作为网络的输入，更多的是采用随机化一个随机初始化的维度较短的词向量？“---->这个只说了一半，应该是随机化一个随机初始化的维度较短的词向量，然后通过训练迭代优化这个词向量。   

“这样做的原因是onehot的词向量维度太大，作为网络输入太复杂？”--->是的，计算量大，性能不好。   

“但是随机初始化的词向量相比onehot词向量输入，是不是收敛性会差一些？”--->如果不通过训练优化词向量，随机初始化的肯定会差的，但是目前通过训练优化词向量是普遍的做法，因此不会出现收敛性差的问题。  


